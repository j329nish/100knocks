{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkxU2mgjHGs2"
   },
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RXmudhKHL1C"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyhaFZ7dHGS6",
    "outputId": "0fcf195c-587c-4bf1-f3c1-e2005b9f7f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/j329nish/.local/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /home/j329nish/.local/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/j329nish/.local/lib/python3.10/site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/j329nish/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mDEPRECATION: lightning-lite 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of lightning-lite or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "/home/j329nish/.local/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "From (redirected): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=b0de3ede-c28a-43b1-b658-b7c6e1b07072\n",
      "To: /home/j329nish/k_home/j329nish/nlplab/100knocks/data/GoogleNews-vectors-negative300.bin.gz\n",
      "100%|██████████████████████████████████████| 1.65G/1.65G [00:33<00:00, 49.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 学習済み単語ベクトルのダウンロード\n",
    "!pip install gdown\n",
    "!gdown --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM -O data/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xoghtUTbwL0",
    "outputId": "0b33141d-6e32-4410-afd9-0f9866031643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
      "Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: lightning-lite 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of lightning-lite or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.20 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "albumentations 1.4.21 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "tensorflow 2.16.1 requires flatbuffers>=23.5.26, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.16.1 requires h5py>=3.10.0, but you have h5py 3.9.0 which is incompatible.\n",
      "tensorflow 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.3 gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "rjdFnaIPaDTC",
    "outputId": "4cfd7949-d2d8-48d5-d5a5-2e06bf424272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (3000001, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# モデルを読み込む\n",
    "model_path = \"./data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "vocab_size = len(model.key_to_index) + 1\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "\n",
    "for idx, word in enumerate(model.key_to_index, start=1):\n",
    "  embedding_matrix[idx] = model[word]\n",
    "  token2id[word] = idx\n",
    "  id2token[idx] = word\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# 属性一覧の表示  https://aiacademy.jp/media/?p=2013\n",
    "# 単語埋め込み    https://nlp100.vercel.app/docs/ch08/ch08.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxgMQps5HNfE"
   },
   "source": [
    "## 71. データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IPB96sTyHEFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-24 00:37:57--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
      "dl.fbaipublicfiles.com (dl.fbaipublicfiles.com) をDNSに問いあわせています... 13.33.5.129, 13.33.5.44, 13.33.5.53, ...\n",
      "dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.33.5.129|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 7439277 (7.1M) [application/zip]\n",
      "`data/SST-2.zip' に保存中\n",
      "\n",
      "SST-2.zip           100%[===================>]   7.09M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-04-24 00:37:57 (78.8 MB/s) - `data/SST-2.zip' へ保存完了 [7439277/7439277]\n",
      "\n",
      "Archive:  data/SST-2.zip\n",
      "  inflating: data/SST-2/dev.tsv      \n",
      "  inflating: data/SST-2/original/README.txt  \n",
      "  inflating: data/SST-2/original/SOStr.txt  \n",
      "  inflating: data/SST-2/original/STree.txt  \n",
      "  inflating: data/SST-2/original/datasetSentences.txt  \n",
      "  inflating: data/SST-2/original/datasetSplit.txt  \n",
      "  inflating: data/SST-2/original/dictionary.txt  \n",
      "  inflating: data/SST-2/original/original_rt_snippets.txt  \n",
      "  inflating: data/SST-2/original/sentiment_labels.txt  \n",
      "  inflating: data/SST-2/test.tsv     \n",
      "  inflating: data/SST-2/train.tsv    \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip -P data/\n",
    "!unzip -o data/SST-2.zip -d data/\n",
    "!rm data/SST-2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データのリスト：\n",
      "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n",
      "{'text': 'contains no wit , only labored gags ', 'label': tensor([0.]), 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
      "{'text': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': tensor([1.]), 'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,\n",
      "         1964])}\n",
      "{'text': 'remains utterly satisfied to remain the same throughout ', 'label': tensor([0.]), 'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}\n",
      "{'text': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': tensor([0.]), 'input_ids': tensor([    6,    12,  1445, 43789,    12, 10946,    76, 41349,    42])}\n",
      "\n",
      "検証データのリスト：\n",
      "{'text': \"it 's a charming and often affecting journey . \", 'label': tensor([1.]), 'input_ids': tensor([   16, 13259,   640,  5199,  3900])}\n",
      "{'text': 'unflinchingly bleak and desperate ', 'label': tensor([0.]), 'input_ids': tensor([136642,  12607,   4984])}\n",
      "{'text': 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ', 'label': tensor([1.]), 'input_ids': tensor([  1488,    165,    684,      4, 953829,      5,   6091,  14671,    339,\n",
      "           513,     15,   1073,    507,  24346,  11212])}\n",
      "{'text': \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \", 'label': tensor([1.]), 'input_ids': tensor([   12,  2527, 10358,   637, 37102,  1868,    20,    53, 18600,   483,\n",
      "           12,   621, 37066, 30723])}\n",
      "{'text': \"it 's slow -- very , very slow . \", 'label': tensor([0.]), 'input_ids': tensor([  16, 1804,  139,  139, 1804])}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import torch\n",
    "\n",
    "def make_dict(file_name):\n",
    "  dictionary = []\n",
    "  with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "      input_ids = [token2id[word] for word in row[0].split() if word in token2id]\n",
    "      if len(input_ids) == 0:\n",
    "        continue\n",
    "      dictionary.append({\n",
    "        'text': row[0], \n",
    "        'label': torch.tensor([float(row[1])]), \n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long)\n",
    "      })\n",
    "  return dictionary\n",
    "\n",
    "train_dict =  make_dict('./data/SST-2/train.tsv')\n",
    "dev_dict = make_dict('./data/SST-2/dev.tsv')\n",
    "\n",
    "print('学習データのリスト：')\n",
    "for i in range(5):\n",
    "  print(train_dict[i])\n",
    "print('')\n",
    "\n",
    "print('検証データのリスト：')\n",
    "for i in range(5):\n",
    "  print(dev_dict[i])\n",
    "\n",
    "# torchのtensorについて https://qiita.com/mathlive/items/241bfb42d852bb801b96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wCuaEPAHRCI"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wTYSlgFIHSUC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "  def __init__(self, embedding_matrix, freeze_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_embedding)\n",
    "    self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = torch.mean(x, dim=0)\n",
    "    y = self.linear(x)\n",
    "    return y\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "\n",
    "# ロジスティック回帰モデルの構築 https://iifx.dev/ja/articles/315600010\n",
    "# torchによるモデルの構築       https://qiita.com/h-hosoda-ml/items/28433b921ed9d3840b03\n",
    "# embeddingのよる単語埋め込み   https://gotutiyan.hatenablog.com/entry/2020/09/02/200144\n",
    "# torchを使った平均ベクトル     https://runebook.dev/ja/articles/pytorch/generated/torch.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wHkw86cHSel"
   },
   "source": [
    "## 73. モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NUG6YkZDHUjH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.4083, dev loss: 0.4717\n",
      "[epoch 2/5] train loss: 0.3815, dev loss: 0.4689\n",
      "[epoch 3/5] train loss: 0.3802, dev loss: 0.4685\n",
      "[epoch 4/5] train loss: 0.3799, dev loss: 0.4684\n",
      "[epoch 5/5] train loss: 0.3798, dev loss: 0.4684\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for train_data in train_dict:\n",
    "    x = train_data['input_ids'].to(device)\n",
    "    y = train_data['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss = []\n",
    "  with torch.no_grad():\n",
    "    for dev_data in dev_dict:\n",
    "      x = dev_data['input_ids'].to(device)\n",
    "      y = dev_data['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "  \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_73.pt\")\n",
    "\n",
    "# モデルの学習 https://iifx.dev/ja/articles/277414431\n",
    "# モデルの保存 https://pystyle.info/pytorch-save-and-load-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYFAHfbKHU3Z"
   },
   "source": [
    "## 74. モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mv0wlmmSHWkj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7935779816513762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = torch.load(\"./model/model_73.pt\").to(device)\n",
    "model.eval()\n",
    "y_pred_list, y_list = [], []\n",
    "for dev_data in dev_dict:\n",
    "  x = dev_data['input_ids'].to(device)\n",
    "  y = float(dev_data['label'].to(device))\n",
    "  y_pred = float(torch.sigmoid(model(x)) > 0.5)\n",
    "  y_list.append(y)\n",
    "  y_pred_list.append(y_pred)\n",
    "\n",
    "print(accuracy_score(y_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goKf0dw1HWuV"
   },
   "source": [
    "## 75. パディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CNyrNz6yHYHu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,\n",
      "           1276,   1964],\n",
      "        [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,\n",
      "              0,      0],\n",
      "        [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,\n",
      "              0,      0],\n",
      "        [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,\n",
      "              0,      0]]), 'label': tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])}\n"
     ]
    }
   ],
   "source": [
    "def collate(data_dict):\n",
    "  data_dict.sort(key=lambda x: len(x['input_ids']), reverse=True)\n",
    "  input_ids_list = [data['input_ids'] for data in data_dict]\n",
    "  label_list = [data['label'] for data in data_dict]\n",
    "  padded_input_ids = nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "  padded_data = {\n",
    "    'input_ids': padded_input_ids,\n",
    "    'label': torch.stack(label_list)\n",
    "  }\n",
    "  return padded_data\n",
    "\n",
    "padded_data = collate(train_dict[0:4])\n",
    "print(padded_data)\n",
    "\n",
    "# ラベルを基準にソート https://www.headboost.jp/python-how-to-sort-a-list/\n",
    "# パディング          https://qiita.com/Y_kazuyan/items/aec5519a81cfd3e98088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ij1XrrHYUP"
   },
   "source": [
    "## 76. ミニバッチ学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rdAvVcGMHZy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.6197, dev loss: 0.5774, acc: 0.7179\n",
      "[epoch 2/5] train loss: 0.5384, dev loss: 0.5272, acc: 0.7569\n",
      "[epoch 3/5] train loss: 0.4963, dev loss: 0.5023, acc: 0.7741\n",
      "[epoch 4/5] train loss: 0.4708, dev loss: 0.4944, acc: 0.7970\n",
      "[epoch 5/5] train loss: 0.4553, dev loss: 0.4858, acc: 0.8005\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ハイパーパラメータ\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "# データの準備\n",
    "train_loader = DataLoader(train_dict, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_dict, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# モデルの定義\n",
    "class LogisticRegression(nn.Module):\n",
    "  def __init__(self, embedding_matrix, freeze_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_embedding)\n",
    "    self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = torch.mean(x, dim=1)\n",
    "    y = self.linear(x)\n",
    "    return y\n",
    "\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# 学習\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_76.pt\")\n",
    "\n",
    "# ミニバッチ学習 https://tma15.github.io/blog/2020/03/10/pytorch%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%82%88%E3%82%8A%E5%B0%91%E3%81%AA%E3%81%84%E3%83%91%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E3%83%9F%E3%83%8B%E3%83%90%E3%83%83%E3%83%81%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/\n",
    "# 複数append    https://af-e.net/python-append-multiple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYs14AP6HaDA"
   },
   "source": [
    "## 77. GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BVQzuyucHbJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.6181, dev loss: 0.5748, acc: 0.7133\n",
      "[epoch 2/5] train loss: 0.5375, dev loss: 0.5261, acc: 0.7534\n",
      "[epoch 3/5] train loss: 0.4960, dev loss: 0.5035, acc: 0.7798\n",
      "[epoch 4/5] train loss: 0.4712, dev loss: 0.4909, acc: 0.7970\n",
      "[epoch 5/5] train loss: 0.4546, dev loss: 0.4823, acc: 0.7993\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_77.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX0US8XJHbXx"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "O2C1VUcdHdZQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.4195, dev loss: 0.4373, acc: 0.8142\n",
      "[epoch 2/5] train loss: 0.2457, dev loss: 0.5252, acc: 0.8119\n",
      "[epoch 3/5] train loss: 0.2076, dev loss: 0.5939, acc: 0.8188\n",
      "[epoch 4/5] train loss: 0.1889, dev loss: 0.6476, acc: 0.8131\n",
      "[epoch 5/5] train loss: 0.1790, dev loss: 0.7154, acc: 0.8096\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(embedding_matrix, False).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_78.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVdHOaSuHdm0"
   },
   "source": [
    "## 79. アーキテクチャの変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ppqs9_xFHgfg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.2740, dev loss: 0.4266, acc: 0.8119\n",
      "[epoch 2/5] train loss: 0.1472, dev loss: 0.5062, acc: 0.8360\n",
      "[epoch 3/5] train loss: 0.1010, dev loss: 0.5242, acc: 0.8257\n",
      "[epoch 4/5] train loss: 0.0770, dev loss: 0.5664, acc: 0.8119\n",
      "[epoch 5/5] train loss: 0.0600, dev loss: 0.7231, acc: 0.8050\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータ\n",
    "batch_size = 32\n",
    "hidden_size = 50\n",
    "num_epochs = 5\n",
    "\n",
    "# データの準備\n",
    "train_loader = DataLoader(train_dict, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_dict, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# モデルの定義\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, embedding_matrix, freeze_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_embedding)\n",
    "    self.num_layers = 2\n",
    "    self.lstm = nn.LSTM(\n",
    "      embedding_matrix.shape[1],\n",
    "      hidden_size, \n",
    "      num_layers=self.num_layers,\n",
    "      batch_first=True,\n",
    "      bidirectional=True\n",
    "    )\n",
    "    self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x, (h,c) = self.lstm(x)\n",
    "    x = torch.cat([h[-2], h[-1]], dim=1)\n",
    "    y = self.linear(x)\n",
    "    return y\n",
    "\n",
    "model = LSTM(embedding_matrix, False).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# 学習\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_79.pt\")\n",
    "\n",
    "# LSTMの作り方1 https://qiita.com/norimaki-senbei/items/186638b558a6dd425921\n",
    "# LSTMの作り方2 https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/\n",
    "# LSTMの作り方3 https://iifx.dev/ja/articles/260750715\n",
    "# 双方向LSTM    https://qiita.com/m__k/items/78a5125d719951ca98d3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

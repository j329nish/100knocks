{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkxU2mgjHGs2"
   },
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RXmudhKHL1C"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyhaFZ7dHGS6",
    "outputId": "0fcf195c-587c-4bf1-f3c1-e2005b9f7f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/j329nish/.local/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in /home/j329nish/.local/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/j329nish/.local/lib/python3.10/site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/j329nish/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mDEPRECATION: lightning-lite 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of lightning-lite or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "/home/j329nish/.local/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Failed to retrieve file url:\n",
      "\n",
      "\tToo many users have viewed or downloaded this file recently. Please\n",
      "\ttry accessing the file again later. If the file you are trying to\n",
      "\taccess is particularly large or is shared with many people, it may\n",
      "\ttake up to 24 hours to be able to view or download the file. If you\n",
      "\tstill can't access a file after 24 hours, contact your domain\n",
      "\tadministrator.\n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\thttps://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "\n",
      "but Gdown can't. Please check connections and permissions.\n"
     ]
    }
   ],
   "source": [
    "# 学習済み単語ベクトルのダウンロード\n",
    "!pip install gdown\n",
    "!gdown --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM -O data/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xoghtUTbwL0",
    "outputId": "0b33141d-6e32-4410-afd9-0f9866031643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.24.3 in /home/j329nish/.local/lib/python3.10/site-packages (1.24.3)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
      "\u001b[33mDEPRECATION: lightning-lite 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of lightning-lite or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.8.0 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.3 gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "rjdFnaIPaDTC",
    "outputId": "4cfd7949-d2d8-48d5-d5a5-2e06bf424272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (3000001, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# モデルを読み込む\n",
    "model_path = \"./data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "vocab_size = len(model.key_to_index) + 1\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "token2id = {\"<PAD>\": 0}\n",
    "id2token = {0: \"<PAD>\"}\n",
    "\n",
    "for idx, word in enumerate(model.key_to_index, start=1):\n",
    "  embedding_matrix[idx] = model[word]\n",
    "  token2id[word] = idx\n",
    "  id2token[idx] = word\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# 属性一覧の表示  https://aiacademy.jp/media/?p=2013\n",
    "# 単語埋め込み    https://nlp100.vercel.app/docs/ch08/ch08.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxgMQps5HNfE"
   },
   "source": [
    "## 71. データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IPB96sTyHEFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-22 20:20:20--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
      "dl.fbaipublicfiles.com (dl.fbaipublicfiles.com) をDNSに問いあわせています... 54.239.168.70, 54.239.168.65, 54.239.168.51, ...\n",
      "dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.239.168.70|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 7439277 (7.1M) [application/zip]\n",
      "`data/SST-2.zip' に保存中\n",
      "\n",
      "SST-2.zip           100%[===================>]   7.09M  32.9MB/s    in 0.2s    \n",
      "\n",
      "2025-04-22 20:20:21 (32.9 MB/s) - `data/SST-2.zip' へ保存完了 [7439277/7439277]\n",
      "\n",
      "Archive:  data/SST-2.zip\n",
      "   creating: data/SST-2/\n",
      "  inflating: data/SST-2/dev.tsv      \n",
      "   creating: data/SST-2/original/\n",
      "  inflating: data/SST-2/original/README.txt  \n",
      "  inflating: data/SST-2/original/SOStr.txt  \n",
      "  inflating: data/SST-2/original/STree.txt  \n",
      "  inflating: data/SST-2/original/datasetSentences.txt  \n",
      "  inflating: data/SST-2/original/datasetSplit.txt  \n",
      "  inflating: data/SST-2/original/dictionary.txt  \n",
      "  inflating: data/SST-2/original/original_rt_snippets.txt  \n",
      "  inflating: data/SST-2/original/sentiment_labels.txt  \n",
      "  inflating: data/SST-2/test.tsv     \n",
      "  inflating: data/SST-2/train.tsv    \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip -P data/\n",
    "!unzip -o data/SST-2.zip -d data/\n",
    "!rm data/SST-2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データのリスト：\n",
      "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n",
      "{'text': 'contains no wit , only labored gags ', 'label': tensor([0.]), 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
      "{'text': 'that loves its characters and communicates something rather beautiful about human nature ', 'label': tensor([1.]), 'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,\n",
      "         1964])}\n",
      "{'text': 'remains utterly satisfied to remain the same throughout ', 'label': tensor([0.]), 'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}\n",
      "{'text': 'on the worst revenge-of-the-nerds clichés the filmmakers could dredge up ', 'label': tensor([0.]), 'input_ids': tensor([    6,    12,  1445, 43789,    12, 10946,    76, 41349,    42])}\n",
      "\n",
      "検証データのリスト：\n",
      "{'text': \"it 's a charming and often affecting journey . \", 'label': tensor([1.]), 'input_ids': tensor([   16, 13259,   640,  5199,  3900])}\n",
      "{'text': 'unflinchingly bleak and desperate ', 'label': tensor([0.]), 'input_ids': tensor([136642,  12607,   4984])}\n",
      "{'text': 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ', 'label': tensor([1.]), 'input_ids': tensor([  1488,    165,    684,      4, 953829,      5,   6091,  14671,    339,\n",
      "           513,     15,   1073,    507,  24346,  11212])}\n",
      "{'text': \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \", 'label': tensor([1.]), 'input_ids': tensor([   12,  2527, 10358,   637, 37102,  1868,    20,    53, 18600,   483,\n",
      "           12,   621, 37066, 30723])}\n",
      "{'text': \"it 's slow -- very , very slow . \", 'label': tensor([0.]), 'input_ids': tensor([  16, 1804,  139,  139, 1804])}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import torch\n",
    "\n",
    "def make_dict(file_name):\n",
    "  dictionary = []\n",
    "  with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "      input_ids = [token2id[word] for word in row[0].split() if word in token2id]\n",
    "      if len(input_ids) == 0:\n",
    "        continue\n",
    "      dictionary.append({\n",
    "        'text': row[0], \n",
    "        'label': torch.tensor([float(row[1])]), \n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long)\n",
    "      })\n",
    "  return dictionary\n",
    "\n",
    "train_dict =  make_dict('./data/SST-2/train.tsv')\n",
    "dev_dict = make_dict('./data/SST-2/dev.tsv')\n",
    "\n",
    "print('学習データのリスト：')\n",
    "for i in range(5):\n",
    "  print(train_dict[i])\n",
    "print('')\n",
    "\n",
    "print('検証データのリスト：')\n",
    "for i in range(5):\n",
    "  print(dev_dict[i])\n",
    "\n",
    "# torchのtensorについて https://qiita.com/mathlive/items/241bfb42d852bb801b96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wCuaEPAHRCI"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "wTYSlgFIHSUC"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "  def __init__(self, embedding_matrix, freeze_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_embedding)\n",
    "    self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = torch.mean(x, dim=0)\n",
    "    y = self.linear(x)\n",
    "    return y\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "\n",
    "# ロジスティック回帰モデルの構築 https://iifx.dev/ja/articles/315600010\n",
    "# torchによるモデルの構築       https://qiita.com/h-hosoda-ml/items/28433b921ed9d3840b03\n",
    "# embeddingのよる単語埋め込み   https://gotutiyan.hatenablog.com/entry/2020/09/02/200144\n",
    "# torchを使った平均ベクトル     https://runebook.dev/ja/articles/pytorch/generated/torch.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wHkw86cHSel"
   },
   "source": [
    "## 73. モデルの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "NUG6YkZDHUjH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 / 5] train loss: 0.4083, dev loss: 0.4717\n",
      "[epoch 2 / 5] train loss: 0.3815, dev loss: 0.4689\n",
      "[epoch 3 / 5] train loss: 0.3802, dev loss: 0.4685\n",
      "[epoch 4 / 5] train loss: 0.3799, dev loss: 0.4684\n",
      "[epoch 5 / 5] train loss: 0.3798, dev loss: 0.4684\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for train_data in train_dict:\n",
    "    x = train_data['input_ids'].to(device)\n",
    "    y = train_data['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss = []\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for dev_data in dev_dict:\n",
    "      x = dev_data['input_ids'].to(device)\n",
    "      y = dev_data['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "  \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_73.pt\")\n",
    "\n",
    "# モデルの学習 https://iifx.dev/ja/articles/277414431\n",
    "# モデルの保存 https://pystyle.info/pytorch-save-and-load-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYFAHfbKHU3Z"
   },
   "source": [
    "## 74. モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Mv0wlmmSHWkj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7935779816513762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = torch.load(\"./model/model_73.pt\").to(device)\n",
    "model.eval()\n",
    "y_pred_list, y_list = [], []\n",
    "for dev_data in dev_dict:\n",
    "  x = dev_data['input_ids'].to(device)\n",
    "  y = float(dev_data['label'].to(device))\n",
    "  y_pred = float(torch.sigmoid(model(x)) > 0.5)\n",
    "  y_list.append(y)\n",
    "  y_pred_list.append(y_pred)\n",
    "\n",
    "print(accuracy_score(y_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goKf0dw1HWuV"
   },
   "source": [
    "## 75. パディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "CNyrNz6yHYHu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
      "        [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
      "        [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
      "        [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]), 'label': tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])}\n"
     ]
    }
   ],
   "source": [
    "def collate(data_dict):\n",
    "  data_dict.sort(key=lambda x: len(x['input_ids']), reverse=True)\n",
    "  input_ids_list = [data['input_ids'] for data in data_dict]\n",
    "  label_list = [data['label'] for data in data_dict]\n",
    "  padded_input_ids = nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "  padded_data = {\n",
    "    'input_ids': padded_input_ids,\n",
    "    'label': torch.stack(label_list)\n",
    "  }\n",
    "  return padded_data\n",
    "\n",
    "padded_data = collate(train_dict[0:4])\n",
    "print(padded_data)\n",
    "\n",
    "# ラベルを基準にソート https://www.headboost.jp/python-how-to-sort-a-list/\n",
    "# パディング          https://qiita.com/Y_kazuyan/items/aec5519a81cfd3e98088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ij1XrrHYUP"
   },
   "source": [
    "## 76. ミニバッチ学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "rdAvVcGMHZy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/10] train loss: 0.6189, dev loss: 0.5747, acc: 0.7064220183486238\n",
      "[epoch 2/10] train loss: 0.5383, dev loss: 0.5266, acc: 0.7557339449541285\n",
      "[epoch 3/10] train loss: 0.4963, dev loss: 0.5055, acc: 0.7889908256880734\n",
      "[epoch 4/10] train loss: 0.4712, dev loss: 0.4911, acc: 0.7958715596330275\n",
      "[epoch 5/10] train loss: 0.4553, dev loss: 0.4825, acc: 0.7993119266055045\n",
      "[epoch 6/10] train loss: 0.4444, dev loss: 0.4803, acc: 0.7993119266055045\n",
      "[epoch 7/10] train loss: 0.4360, dev loss: 0.4765, acc: 0.8004587155963303\n",
      "[epoch 8/10] train loss: 0.4296, dev loss: 0.4770, acc: 0.7993119266055045\n",
      "[epoch 9/10] train loss: 0.4255, dev loss: 0.4739, acc: 0.805045871559633\n",
      "[epoch 10/10] train loss: 0.4214, dev loss: 0.4729, acc: 0.8027522935779816\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ハイパーパラメータ\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "\n",
    "# データの準備\n",
    "train_loader = DataLoader(train_dict, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_dict, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# モデルの定義\n",
    "class LogisticRegression(nn.Module):\n",
    "  def __init__(self, embedding_matrix, freeze_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_embedding)\n",
    "    self.linear = nn.Linear(embedding_matrix.shape[1], 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = torch.mean(x, dim=1)\n",
    "    y = self.linear(x)\n",
    "    return y\n",
    "\n",
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# 学習\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_76.pt\")\n",
    "\n",
    "# ミニバッチ学習 https://tma15.github.io/blog/2020/03/10/pytorch%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-%E3%82%88%E3%82%8A%E5%B0%91%E3%81%AA%E3%81%84%E3%83%91%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E3%81%A7%E3%83%9F%E3%83%8B%E3%83%90%E3%83%83%E3%83%81%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/\n",
    "# 複数append    https://af-e.net/python-append-multiple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYs14AP6HaDA"
   },
   "source": [
    "## 77. GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "BVQzuyucHbJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/10] train loss: 0.6205, dev loss: 0.5760, acc: 0.7087155963302753\n",
      "[epoch 2/10] train loss: 0.5382, dev loss: 0.5269, acc: 0.7568807339449541\n",
      "[epoch 3/10] train loss: 0.4962, dev loss: 0.5035, acc: 0.7821100917431193\n",
      "[epoch 4/10] train loss: 0.4710, dev loss: 0.4950, acc: 0.7970183486238532\n",
      "[epoch 5/10] train loss: 0.4550, dev loss: 0.4847, acc: 0.8004587155963303\n",
      "[epoch 6/10] train loss: 0.4436, dev loss: 0.4821, acc: 0.7970183486238532\n",
      "[epoch 7/10] train loss: 0.4360, dev loss: 0.4758, acc: 0.801605504587156\n",
      "[epoch 8/10] train loss: 0.4300, dev loss: 0.4731, acc: 0.801605504587156\n",
      "[epoch 9/10] train loss: 0.4245, dev loss: 0.4753, acc: 0.801605504587156\n",
      "[epoch 10/10] train loss: 0.4221, dev loss: 0.4734, acc: 0.8038990825688074\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(embedding_matrix, True).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_77.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX0US8XJHbXx"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "O2C1VUcdHdZQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.4190, dev loss: 0.4599, acc: 0.8131\n",
      "[epoch 2/5] train loss: 0.2451, dev loss: 0.5311, acc: 0.8177\n",
      "[epoch 3/5] train loss: 0.2080, dev loss: 0.6064, acc: 0.8154\n",
      "[epoch 4/5] train loss: 0.1888, dev loss: 0.6640, acc: 0.8073\n",
      "[epoch 5/5] train loss: 0.1775, dev loss: 0.7470, acc: 0.8039\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(embedding_matrix, False).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_78.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVdHOaSuHdm0"
   },
   "source": [
    "## 79. アーキテクチャの変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "Ppqs9_xFHgfg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1/5] train loss: 0.2747, dev loss: 0.4014, acc: 0.8245\n",
      "[epoch 2/5] train loss: 0.1483, dev loss: 0.4663, acc: 0.8142\n",
      "[epoch 3/5] train loss: 0.1033, dev loss: 0.5459, acc: 0.8234\n",
      "[epoch 4/5] train loss: 0.0781, dev loss: 0.6023, acc: 0.8165\n",
      "[epoch 5/5] train loss: 0.0617, dev loss: 0.7633, acc: 0.8119\n"
     ]
    }
   ],
   "source": [
    "# ハイパーパラメータ\n",
    "batch_size = 32\n",
    "hidden_size = 50\n",
    "num_epochs = 5\n",
    "\n",
    "# データの準備\n",
    "train_loader = DataLoader(train_dict, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_dict, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# モデルの定義\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, embedding_matrix, freeze_embedding):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=freeze_embedding)\n",
    "    self.num_layers = 2\n",
    "    self.lstm = nn.LSTM(\n",
    "      embedding_matrix.shape[1],\n",
    "      hidden_size, \n",
    "      num_layers=self.num_layers,\n",
    "      batch_first=True,\n",
    "      bidirectional=True\n",
    "    )\n",
    "    self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x, (h,c) = self.lstm(x)\n",
    "    x = torch.cat([h[-2], h[-1]], dim=1)\n",
    "    y = self.linear(x)\n",
    "    return y\n",
    "\n",
    "model = LSTM(embedding_matrix, False).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# 学習\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = []\n",
    "  for batch in train_loader:\n",
    "    x = batch['input_ids'].to(device)\n",
    "    y = batch['label'].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "  model.eval()\n",
    "  dev_loss, y_pred_list, y_list = [], [], []\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dev_loader:\n",
    "      x = batch['input_ids'].to(device)\n",
    "      y = batch['label'].to(device)\n",
    "      y_pred = model(x)\n",
    "      loss = criterion(y_pred, y)\n",
    "      dev_loss.append(loss.item())\n",
    "      y_list.extend(y.cpu().numpy())\n",
    "      y_pred_list.extend((y_pred > 0.5).float().cpu().numpy())\n",
    "    \n",
    "  print(f'[epoch {epoch + 1}/{num_epochs}] train loss: {np.mean(train_loss):.4f}, dev loss: {np.mean(dev_loss):.4f}, acc: {accuracy_score(y_list, y_pred_list):.4f}')\n",
    "\n",
    "torch.save(model, \"./model/model_79.pt\")\n",
    "\n",
    "# LSTMの作り方1 https://qiita.com/norimaki-senbei/items/186638b558a6dd425921\n",
    "# LSTMの作り方2 https://tma15.github.io/blog/2020/03/15/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86-lstm%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AD%A6%E7%BF%92-pytorch%E3%82%B3%E3%83%BC%E3%83%89%E4%BB%98%E3%81%8D/\n",
    "# LSTMの作り方3 https://iifx.dev/ja/articles/260750715\n",
    "# 双方向LSTM    https://qiita.com/m__k/items/78a5125d719951ca98d3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

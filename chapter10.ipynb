{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr7Z_an-gIFd"
   },
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-OaV13HgLMK"
   },
   "source": [
    "## 90. 次単語予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn6nyNI0gK5L",
    "outputId": "69a29b3e-0d9f-4d35-a0d0-0c750df62bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 464, 3807,  373, 1336,  286]])\n",
      " jokes: 0.0219\n",
      " great: 0.0186\n",
      " laughs: 0.0115\n",
      " bad: 0.0109\n",
      " surprises: 0.0107\n",
      " references: 0.0105\n",
      " fun: 0.0100\n",
      " humor: 0.0074\n",
      " \": 0.0074\n",
      " the: 0.0067\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(42)\n",
    "text = \"The movie was full of\"\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "print(\"input_ids:\", input_ids)\n",
    "\n",
    "# 次のトークンの予測確率を取得\n",
    "with torch.no_grad():  \n",
    "  output = model(input_ids)\n",
    "  next_token_logits = output.logits[0,-1,:]\n",
    "\n",
    "# 確率を計算\n",
    "scores = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# 上位10個を出力\n",
    "topk = 10\n",
    "topk_scores, topk_ids = torch.topk(scores, topk)\n",
    "for topk_score, topk_id in zip(topk_scores, topk_ids):\n",
    "  pred_token = tokenizer.decode([topk_id])\n",
    "  print(f'{pred_token}: {topk_score:.4f}')\n",
    "\n",
    "# GPT型の使い方    https://qiita.com/suzuki_sh/items/acf276b55085647bdd75\n",
    "# CausalLMOutput  https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBdyjVqSgM3k"
   },
   "source": [
    "## 91. 続きのテキストの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "URf1Fbg_e3ZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.2, topk=10: The movie was full of great moments, but it was also full of bad moments.\n",
      "\n",
      "The\n",
      "temp=0.4, topk=20: The movie was full of good, bad, and ugly jokes. I was so happy with this movie\n",
      "temp=0.6, topk=30: The movie was full of great characters and all of the great roles that were played by great actresses.\n",
      "temp=0.8, topk=40: The movie was full of jokes, but it did have some serious undertones of sexism and racism.\n",
      "temp=1.0, topk=50: The movie was full of cringe. I didn't even get a reaction from any of the movie's\n"
     ]
    }
   ],
   "source": [
    "temp_list = [t * 0.2 for t in range(1, 6)]\n",
    "topk_list = [k * 10 for k in range(1, 6)]\n",
    "\n",
    "with torch.no_grad():\n",
    "  for temp, topk in zip(temp_list, topk_list):\n",
    "    output_ids = model.generate(\n",
    "      input_ids,\n",
    "      do_sample=True,\n",
    "      temperature=temp,\n",
    "      top_k=topk,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    preds = tokenizer.decode(output_ids.tolist()[0])\n",
    "    print(f'temp={temp:.1f}, topk={topk}: {preds}')\n",
    "\n",
    "# gptのtemperature  https://qiita.com/suzuki_sh/items/8e449d231bb2f09a510c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdWY_WN4gOJM"
   },
   "source": [
    "## 92. 予測されたテキストの確率を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "mAK6MaX_gPue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jokes: 0.0219\n",
      " and: 0.2892\n",
      " jokes: 0.0985\n",
      " about: 0.2056\n",
      " how: 0.0997\n",
      " the: 0.0846\n",
      " movie: 0.0364\n",
      " was: 0.2963\n",
      " a: 0.0677\n",
      " joke: 0.1735\n",
      ".: 0.2804\n",
      " It: 0.1230\n",
      " was: 0.5197\n",
      " a: 0.1493\n",
      " joke: 0.2690\n"
     ]
    }
   ],
   "source": [
    "# 次のトークンの予測確率を取得\n",
    "with torch.no_grad():\n",
    "  output_ids = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id)\n",
    "  generated_tokens_ids = output_ids[0, input_ids.shape[1]:]\n",
    "  output = model(output_ids)\n",
    "  next_text_logits = output.logits[0, input_ids.shape[1]-1:, :]\n",
    "\n",
    "# 各トークンの確率を計算\n",
    "scores = torch.softmax(next_text_logits, dim=-1)\n",
    "for i, token_id in enumerate(generated_tokens_ids):\n",
    "  print(f'{tokenizer.decode([token_id])}: {scores[i, token_id]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na9dvEJUgP9c"
   },
   "source": [
    "## 93. パープレキシティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "SI614KWcgRLY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of surprises: 99.3548\n",
      "The movies were full of surprises: 126.4808\n",
      "The movie were full of surprises: 278.8784\n",
      "The movies was full of surprises: 274.6573\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "  \"The movie was full of surprises\",\n",
    "  \"The movies were full of surprises\",\n",
    "  \"The movie were full of surprises\",\n",
    "  \"The movies was full of surprises\"\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "with torch.no_grad():\n",
    "  outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['input_ids'])\n",
    "\n",
    "# パープレキシティの計算\n",
    "shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs['input_ids'][:, 1:].contiguous()\n",
    "shift_mask = inputs['attention_mask'][:, 1:].contiguous()\n",
    "batch_size, seq_len = shift_labels.shape\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).view(batch_size, seq_len)\n",
    "loss = (loss * shift_mask).sum(dim=1) / shift_mask.sum(dim=1)\n",
    "ppl = torch.exp(loss).tolist()\n",
    "\n",
    "for i in range(len(texts)):\n",
    "  print(f'{texts[i]}: {ppl[i]:.4f}')\n",
    "\n",
    "# パープレキシティの計算  https://gotutiyan.hatenablog.com/entry/2022/02/23/133414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtTn1lpFgRYb"
   },
   "source": [
    "## 94. チャットテンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQQV-4OIgSbY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKr1YHw1gSnP"
   },
   "source": [
    "## 95. マルチターンのチャット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsLXc3Y2gTt9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTU_FE7FgT2x"
   },
   "source": [
    "## 96. プロンプトによる感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kau4qtt7gU8T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca6DMpLWgWAv"
   },
   "source": [
    "## 97. 埋め込みに基づく感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCDrDp0AgXbC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSIf41cMgX6i"
   },
   "source": [
    "## 98. ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYLN8VTcgZH-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdGNChqpgZVA"
   },
   "source": [
    "## 99. 選好チューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0U1XT20gacr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

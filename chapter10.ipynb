{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr7Z_an-gIFd"
   },
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-OaV13HgLMK"
   },
   "source": [
    "## 90. 次単語予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in ./.venv/lib/python3.10/site-packages (from transformers[torch]) (2.7.0)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0->transformers[torch]) (80.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2025.4.26)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: transformers, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [accelerate]2\u001b[0m [accelerate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn6nyNI0gK5L",
    "outputId": "69a29b3e-0d9f-4d35-a0d0-0c750df62bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 464, 3807,  373, 1336,  286]])\n",
      " jokes: 0.0219\n",
      " great: 0.0186\n",
      " laughs: 0.0115\n",
      " bad: 0.0109\n",
      " surprises: 0.0107\n",
      " references: 0.0105\n",
      " fun: 0.0100\n",
      " humor: 0.0074\n",
      " \": 0.0074\n",
      " the: 0.0067\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(42)\n",
    "text = \"The movie was full of\"\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "print(\"input_ids:\", input_ids)\n",
    "\n",
    "# 次のトークンの予測確率を取得\n",
    "with torch.no_grad():  \n",
    "  output = model(input_ids)\n",
    "  next_token_logits = output.logits[0,-1,:]\n",
    "\n",
    "# 確率を計算\n",
    "scores = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# 上位10個を出力\n",
    "topk = 10\n",
    "topk_scores, topk_ids = torch.topk(scores, topk)\n",
    "for topk_score, topk_id in zip(topk_scores, topk_ids):\n",
    "  pred_token = tokenizer.decode([topk_id])\n",
    "  print(f'{pred_token}: {topk_score:.4f}')\n",
    "\n",
    "# GPT型の使い方    https://qiita.com/suzuki_sh/items/acf276b55085647bdd75\n",
    "# CausalLMOutput  https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBdyjVqSgM3k"
   },
   "source": [
    "## 91. 続きのテキストの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "URf1Fbg_e3ZV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.2, topk=10: The movie was full of surprises, but it was also a great experience. I was able to get a lot of laughs out\n",
      "temp=0.4, topk=20: The movie was full of hilarious moments, like the one where the character is trying to kill a girl who is trying to kill\n",
      "temp=0.6, topk=30: The movie was full of jokes, jokes about women, jokes about men and so on, and I think it's a really\n",
      "temp=0.8, topk=40: The movie was full of references to the history of the US military and their ability to defeat North Korea. The film was called\n",
      "temp=1.0, topk=50: The movie was full of people complaining about the way the characters behaved towards other people, but it didn't have an obvious gender\n"
     ]
    }
   ],
   "source": [
    "temp_list = [t * 0.2 for t in range(1, 6)]\n",
    "topk_list = [k * 10 for k in range(1, 6)]\n",
    "\n",
    "with torch.no_grad():\n",
    "  for temp, topk in zip(temp_list, topk_list):\n",
    "    output_ids = model.generate(\n",
    "      input_ids,\n",
    "      do_sample=True,\n",
    "      temperature=temp,\n",
    "      top_k=topk,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    preds = tokenizer.decode(output_ids.tolist()[0])\n",
    "    print(f'temp={temp:.1f}, topk={topk}: {preds}')\n",
    "\n",
    "# gptのtemperature  https://qiita.com/suzuki_sh/items/8e449d231bb2f09a510c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdWY_WN4gOJM"
   },
   "source": [
    "## 92. 予測されたテキストの確率を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mAK6MaX_gPue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jokes: 0.0219\n",
      " and: 0.2892\n",
      " jokes: 0.0985\n",
      " about: 0.2056\n",
      " how: 0.0997\n",
      " the: 0.0846\n",
      " movie: 0.0364\n",
      " was: 0.2963\n",
      " a: 0.0677\n",
      " joke: 0.1735\n",
      ".: 0.2804\n",
      " It: 0.1230\n",
      " was: 0.5197\n",
      " a: 0.1493\n",
      " joke: 0.2690\n",
      " about: 0.4242\n",
      " how: 0.1742\n",
      " the: 0.1236\n",
      " movie: 0.6161\n",
      " was: 0.6350\n"
     ]
    }
   ],
   "source": [
    "# 次のトークンの予測確率を取得\n",
    "with torch.no_grad():\n",
    "  output_ids = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id)\n",
    "  generated_tokens_ids = output_ids[0, input_ids.shape[1]:]\n",
    "  output = model(output_ids)\n",
    "  next_text_logits = output.logits[0, input_ids.shape[1]-1:, :]\n",
    "\n",
    "# 各トークンの確率を計算\n",
    "scores = torch.softmax(next_text_logits, dim=-1)\n",
    "for i, token_id in enumerate(generated_tokens_ids):\n",
    "  print(f'{tokenizer.decode([token_id])}: {scores[i, token_id]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na9dvEJUgP9c"
   },
   "source": [
    "## 93. パープレキシティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SI614KWcgRLY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of surprises: 99.3539\n",
      "The movies were full of surprises: 126.4818\n",
      "The movie were full of surprises: 278.8779\n",
      "The movies was full of surprises: 274.6610\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "  \"The movie was full of surprises\",\n",
    "  \"The movies were full of surprises\",\n",
    "  \"The movie were full of surprises\",\n",
    "  \"The movies was full of surprises\"\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "with torch.no_grad():\n",
    "  outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['input_ids'])\n",
    "\n",
    "# パープレキシティの計算\n",
    "shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs['input_ids'][:, 1:].contiguous()\n",
    "shift_mask = inputs['attention_mask'][:, 1:].contiguous()\n",
    "batch_size, seq_len = shift_labels.shape\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).view(batch_size, seq_len)\n",
    "loss = (loss * shift_mask).sum(dim=1) / shift_mask.sum(dim=1)\n",
    "ppl = torch.exp(loss).tolist()\n",
    "\n",
    "for i in range(len(texts)):\n",
    "  print(f'{texts[i]}: {ppl[i]:.4f}')\n",
    "\n",
    "# パープレキシティの計算  https://gotutiyan.hatenablog.com/entry/2022/02/23/133414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtTn1lpFgRYb"
   },
   "source": [
    "## 94. チャットテンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DQQV-4OIgSbY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sweet treat after dinner is often referred to as dessert.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "\n",
    "prompt = \"What do you call a sweet eaten after dinner?\"\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512, pad_token_id=128001)\n",
    "generated_ids = [\n",
    "  output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKr1YHw1gSnP"
   },
   "source": [
    "## 95. マルチターンのチャット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plural form of the word \"dessert\" is \"desserts\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please give me the plural form of the word with its spelling in reverse order.\"\n",
    "messages.append({\"role\": \"assistant\", \"content\":response})\n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512, pad_token_id=128001)\n",
    "generated_ids = [\n",
    "  output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTU_FE7FgT2x"
   },
   "source": [
    "## 96. プロンプトによる感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kau4qtt7gU8T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-17 19:48:12--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.65.112.7, 18.65.112.6, 18.65.112.33, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.65.112.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7439277 (7.1M) [application/zip]\n",
      "Saving to: ‘data/SST-2.zip’\n",
      "\n",
      "SST-2.zip           100%[===================>]   7.09M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-05-17 19:48:12 (49.6 MB/s) - ‘data/SST-2.zip’ saved [7439277/7439277]\n",
      "\n",
      "Archive:  data/SST-2.zip\n",
      "  inflating: data/SST-2/dev.tsv      \n",
      "  inflating: data/SST-2/original/README.txt  \n",
      "  inflating: data/SST-2/original/SOStr.txt  \n",
      "  inflating: data/SST-2/original/STree.txt  \n",
      "  inflating: data/SST-2/original/datasetSentences.txt  \n",
      "  inflating: data/SST-2/original/datasetSplit.txt  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  inflating: data/SST-2/original/dictionary.txt  \n",
      "  inflating: data/SST-2/original/original_rt_snippets.txt  \n",
      "  inflating: data/SST-2/original/sentiment_labels.txt  \n",
      "  inflating: data/SST-2/test.tsv     \n",
      "  inflating: data/SST-2/train.tsv    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip -P data/\n",
    "!unzip -o data/SST-2.zip -d data/\n",
    "!rm data/SST-2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./.venv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [multidict]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.3.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling fsspec-2025.3.2:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.3.2━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [multidict]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [datasets]/17\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.41%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ファイルの読み込み\n",
    "file_name = './data/SST-2/dev.tsv'\n",
    "df = pd.read_csv(file_name, sep='\\t')\n",
    "\n",
    "# 一文に対しての感情分析\n",
    "def sentiment_analysis(text):\n",
    "  instructions = \"\"\"\n",
    "    Please determine the positive and negative aspects of the text. \n",
    "    If it's positive, output 1, if negative, output 0.\n",
    "    You can only output 0 or 1.\n",
    "  \"\"\"\n",
    "  prompt = f\"\"\"\n",
    "    Instructions: {instructions},\n",
    "    Text: {text}\n",
    "  \"\"\"\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You can only output 0 or 1.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "  \n",
    "  text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=512, pad_token_id=128001)\n",
    "  generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "  ]\n",
    "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "  return response\n",
    "\n",
    "# 正解率の計算\n",
    "correct = 0\n",
    "for index, row in df.iterrows():\n",
    "  response = sentiment_analysis(row['sentence'])\n",
    "  if re.search(r\"\\b[01]\\b\", response) and int(re.findall(r\"\\b[01]\\b\", response)[0]) == row['label']:\n",
    "    correct += 1\n",
    "print(f\"accuracy: {correct / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca6DMpLWgWAv"
   },
   "source": [
    "## 97. 埋め込みに基づく感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCDrDp0AgXbC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e050e68158e640329980a455061f6a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a758e8a3d53b44fe924efaf5f45bcb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas5/data/home/nishida/b4/nlp-100knock/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5265' max='5265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5265/5265 29:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.363800</td>\n",
       "      <td>0.238002</td>\n",
       "      <td>0.919725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.258629</td>\n",
       "      <td>0.913991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162400</td>\n",
       "      <td>0.247601</td>\n",
       "      <td>0.922018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.254076</td>\n",
       "      <td>0.923165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.271663</td>\n",
       "      <td>0.922018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/nas5/data/home/nishida/b4/nlp-100knock/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas5/data/home/nishida/b4/nlp-100knock/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas5/data/home/nishida/b4/nlp-100knock/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas5/data/home/nishida/b4/nlp-100knock/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/net/nas5/data/home/nishida/b4/nlp-100knock/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2540757358074188, 'eval_accuracy': 0.9231651376146789, 'eval_runtime': 2.0523, 'eval_samples_per_second': 424.879, 'eval_steps_per_second': 6.821, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "  AutoModelForSequenceClassification,\n",
    "  BatchEncoding, \n",
    "  DataCollatorWithPadding,\n",
    "  TrainingArguments,\n",
    "  Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# ファイルの読み込み\n",
    "train_file_name = './data/SST-2/train.tsv'\n",
    "dev_file_name = './data/SST-2/dev.tsv'\n",
    "train_df = pd.read_csv(train_file_name, sep='\\t', header=0)\n",
    "dev_df = pd.read_csv(dev_file_name, sep='\\t', header=0)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "# モデルの読み込み\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 前処理\n",
    "def preprocess_text_classification(example: dict[str, str | int]) -> BatchEncoding:\n",
    "  encoded_example = tokenizer(example[\"sentence\"], truncation=True, max_length=512)\n",
    "  encoded_example[\"labels\"] = example[\"label\"]\n",
    "  return encoded_example\n",
    "encoded_train_dataset = train_dataset.map(\n",
    "  preprocess_text_classification, remove_columns=train_dataset.column_names\n",
    ")\n",
    "encoded_dev_dataset = dev_dataset.map(\n",
    "  preprocess_text_classification, remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"model/model_97\",\n",
    "  per_device_train_batch_size=32,\n",
    "  per_device_eval_batch_size=32,\n",
    "  learning_rate=2e-5,\n",
    "  lr_scheduler_type=\"linear\",\n",
    "  warmup_ratio=0.1,\n",
    "  num_train_epochs=5,\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"epoch\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "def compute_accuracy(eval_pred: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "  predictions, labels = eval_pred\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# モデルの評価\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSIf41cMgX6i"
   },
   "source": [
    "## 98. ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1c7c5efff246b586ce7d11116654d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a9f7aea25146029fc00cdc3c9ccd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42095' max='42095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42095/42095 30:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>1.009285</td>\n",
       "      <td>0.919725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>1.073673</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>1.150777</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>1.192434</td>\n",
       "      <td>0.916284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>1.217567</td>\n",
       "      <td>0.915138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0092854499816895, 'eval_accuracy': 0.9197247706422018, 'eval_runtime': 11.2422, 'eval_samples_per_second': 77.565, 'eval_steps_per_second': 9.696, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# プロンプトの追加\n",
    "def add_prompt(text):\n",
    "  instructions = \"\"\"\n",
    "    Please determine the positive and negative aspects of the text. \n",
    "    If it's positive, output negative, if negative, output positive.\n",
    "    You can only output negative or positive.\n",
    "  \"\"\"\n",
    "  prompt = f\"\"\"\n",
    "    Instructions: {instructions},\n",
    "    Text: {text}\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return prompt\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "# モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 前処理\n",
    "def preprocess_text_classification(example: dict[str, str | int]) -> BatchEncoding:\n",
    "  sentences = [add_prompt(text) for text in example[\"sentence\"]]\n",
    "  labels = [\"positive\" if label == 1 else \"negative\" for label in example[\"label\"]]\n",
    "  texts = [sentence + label for sentence, label in zip(sentences, labels)]\n",
    "  inputs = tokenizer(sentences, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  inputs[\"labels\"] = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)[\"input_ids\"]\n",
    "  inputs[\"labels\"] = [\n",
    "    [(token if token != tokenizer.pad_token_id else -100) for token in input_ids]\n",
    "    for input_ids in inputs[\"labels\"]\n",
    "  ]\n",
    "  return inputs\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(\n",
    "  preprocess_text_classification, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "encoded_dev_dataset = dev_dataset.map(\n",
    "  preprocess_text_classification, batched=True, remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"model/model_98\",\n",
    "  per_device_train_batch_size=8,\n",
    "  per_device_eval_batch_size=8,\n",
    "  learning_rate=2e-5,\n",
    "  lr_scheduler_type=\"linear\",\n",
    "  warmup_ratio=0.1,\n",
    "  num_train_epochs=5,\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"epoch\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"accuracy\",\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "# 評価関数の作成（predictionとlabelのindexを合わせる）\n",
    "def compute_accuracy(eval_pred: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  label_index_list = []\n",
    "  for i in range(labels.shape[0]):\n",
    "    label_index_list.append(np.where(labels[i] != -100)[0][-1])\n",
    "  pred_labels = predictions[np.arange(labels.shape[0]), np.array(label_index_list)]\n",
    "  true_labels = labels[np.arange(labels.shape[0]), np.array(label_index_list)]\n",
    "  return {\"accuracy\": (pred_labels == true_labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  train_dataset=encoded_train_dataset,\n",
    "  eval_dataset=encoded_dev_dataset,\n",
    "  data_collator=data_collator,\n",
    "  args=training_args,\n",
    "  compute_metrics=compute_accuracy,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# モデルの評価\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdGNChqpgZVA"
   },
   "source": [
    "## 99. 選好チューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe537abf09f4b7189763a3d9affb3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6ee5892c304ef993c12a5d0a17e10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4232d5ccf5a940eea9e5e5b2c77084bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b327a4bcaf14493eb25745be8e52c009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa6c5c5453f415587852f35aed39598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857cf53d428d44f9941c959e735ac81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad207d84cd9f4dcd9d3764d3929d19ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcf16b281414757877daa7ab6b12b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 26:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.246900</td>\n",
       "      <td>0.713921</td>\n",
       "      <td>-7.919707</td>\n",
       "      <td>-14.764617</td>\n",
       "      <td>0.902523</td>\n",
       "      <td>6.844910</td>\n",
       "      <td>-211.613831</td>\n",
       "      <td>-324.974060</td>\n",
       "      <td>-85.944031</td>\n",
       "      <td>-85.876801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.519216</td>\n",
       "      <td>-1.016045</td>\n",
       "      <td>-8.202286</td>\n",
       "      <td>0.919725</td>\n",
       "      <td>7.186241</td>\n",
       "      <td>-142.577225</td>\n",
       "      <td>-259.350739</td>\n",
       "      <td>-98.998993</td>\n",
       "      <td>-98.446442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.550851</td>\n",
       "      <td>-0.543727</td>\n",
       "      <td>-8.290795</td>\n",
       "      <td>0.911697</td>\n",
       "      <td>7.747069</td>\n",
       "      <td>-137.854034</td>\n",
       "      <td>-260.235840</td>\n",
       "      <td>-100.604843</td>\n",
       "      <td>-100.222481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5192157626152039, 'eval_runtime': 3.7994, 'eval_samples_per_second': 229.51, 'eval_steps_per_second': 28.689, 'eval_rewards/chosen': -1.016045093536377, 'eval_rewards/rejected': -8.202285766601562, 'eval_rewards/accuracies': 0.9197247624397278, 'eval_rewards/margins': 7.1862406730651855, 'eval_logps/chosen': -142.5772247314453, 'eval_logps/rejected': -259.3507385253906, 'eval_logits/chosen': -98.99899291992188, 'eval_logits/rejected': -98.44644165039062, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "from trl.trainer.utils import DPODataCollatorWithPadding\n",
    "\n",
    "# 前処理\n",
    "def convert_to_dpo_format(example: dict) -> dict:\n",
    "  if example[\"label\"] == 1:\n",
    "    chosen = example[\"sentence\"] + \"positive\"\n",
    "    rejected = example[\"sentence\"] + \"negative\"\n",
    "  else:\n",
    "    chosen = example[\"sentence\"] + \"negative\"\n",
    "    rejected = example[\"sentence\"] + \"positive\"\n",
    "  return {\"prompt\": example[\"sentence\"], \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_df)\n",
    "dev_dataset = Dataset.from_dict(dev_df)\n",
    "train_dataset = train_dataset.map(\n",
    "  convert_to_dpo_format, remove_columns=train_dataset.column_names\n",
    ")\n",
    "dev_dataset = dev_dataset.map(\n",
    "  convert_to_dpo_format, remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "# モデルの学習\n",
    "data_collator = DPODataCollatorWithPadding()\n",
    "dpo_config = DPOConfig(\n",
    "  output_dir=\"model/model_99\",\n",
    "  per_device_train_batch_size=8,\n",
    "  per_device_eval_batch_size=8,\n",
    "  learning_rate=2e-5,\n",
    "  lr_scheduler_type=\"linear\",\n",
    "  warmup_ratio=0.1,\n",
    "  num_train_epochs=3,\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"epoch\",\n",
    "  eval_strategy=\"epoch\",\n",
    "  load_best_model_at_end=True,\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "  model=model,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=dev_dataset,\n",
    "  args=dpo_config,\n",
    "  processing_class=tokenizer\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# モデルの評価\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr7Z_an-gIFd"
   },
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-OaV13HgLMK"
   },
   "source": [
    "## 90. 次単語予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn6nyNI0gK5L",
    "outputId": "69a29b3e-0d9f-4d35-a0d0-0c750df62bfe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishida/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 464, 3807,  373, 1336,  286]])\n",
      " jokes: 0.0219\n",
      " great: 0.0186\n",
      " laughs: 0.0115\n",
      " bad: 0.0109\n",
      " surprises: 0.0107\n",
      " references: 0.0105\n",
      " fun: 0.0100\n",
      " humor: 0.0074\n",
      " \": 0.0074\n",
      " the: 0.0067\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(42)\n",
    "text = \"The movie was full of\"\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "print(\"input_ids:\", input_ids)\n",
    "\n",
    "# 次のトークンの予測確率を取得\n",
    "with torch.no_grad():  \n",
    "  output = model(input_ids)\n",
    "  next_token_logits = output.logits[0,-1,:]\n",
    "\n",
    "# 確率を計算\n",
    "scores = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# 上位10個を出力\n",
    "topk = 10\n",
    "topk_scores, topk_ids = torch.topk(scores, topk)\n",
    "for topk_score, topk_id in zip(topk_scores, topk_ids):\n",
    "  pred_token = tokenizer.decode([topk_id])\n",
    "  print(f'{pred_token}: {topk_score:.4f}')\n",
    "\n",
    "# GPT型の使い方    https://qiita.com/suzuki_sh/items/acf276b55085647bdd75\n",
    "# CausalLMOutput  https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBdyjVqSgM3k"
   },
   "source": [
    "## 91. 続きのテキストの予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "URf1Fbg_e3ZV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp=0.2, topk=10: The movie was full of great moments, but the most memorable was when the villain, the villainous villain, was killed by\n",
      "temp=0.4, topk=20: The movie was full of laughs, but I was disappointed that it didn't get a lot of attention. The movie was a\n",
      "temp=0.6, topk=30: The movie was full of a lot of weird, bizarre and terrible things to say, and I think that's why I love\n",
      "temp=0.8, topk=40: The movie was full of jokes.\n",
      "\n",
      "\"I was playing in the bathtub, and I felt a little nervous,\"\n",
      "temp=1.0, topk=50: The movie was full of strong performances and a big smile.\n",
      "\n",
      "When I saw the synopsis for this movie, I expected\n"
     ]
    }
   ],
   "source": [
    "temp_list = [t * 0.2 for t in range(1, 6)]\n",
    "topk_list = [k * 10 for k in range(1, 6)]\n",
    "\n",
    "with torch.no_grad():\n",
    "  for temp, topk in zip(temp_list, topk_list):\n",
    "    output_ids = model.generate(\n",
    "      input_ids,\n",
    "      do_sample=True,\n",
    "      temperature=temp,\n",
    "      top_k=topk,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    preds = tokenizer.decode(output_ids.tolist()[0])\n",
    "    print(f'temp={temp:.1f}, topk={topk}: {preds}')\n",
    "\n",
    "# gptのtemperature  https://qiita.com/suzuki_sh/items/8e449d231bb2f09a510c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdWY_WN4gOJM"
   },
   "source": [
    "## 92. 予測されたテキストの確率を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mAK6MaX_gPue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " jokes: 0.0219\n",
      " and: 0.2892\n",
      " jokes: 0.0985\n",
      " about: 0.2056\n",
      " how: 0.0997\n",
      " the: 0.0846\n",
      " movie: 0.0364\n",
      " was: 0.2963\n",
      " a: 0.0677\n",
      " joke: 0.1735\n",
      ".: 0.2804\n",
      " It: 0.1230\n",
      " was: 0.5197\n",
      " a: 0.1493\n",
      " joke: 0.2690\n",
      " about: 0.4242\n",
      " how: 0.1742\n",
      " the: 0.1236\n",
      " movie: 0.6161\n",
      " was: 0.6350\n"
     ]
    }
   ],
   "source": [
    "# 次のトークンの予測確率を取得\n",
    "with torch.no_grad():\n",
    "  output_ids = model.generate(input_ids, pad_token_id=tokenizer.eos_token_id)\n",
    "  generated_tokens_ids = output_ids[0, input_ids.shape[1]:]\n",
    "  output = model(output_ids)\n",
    "  next_text_logits = output.logits[0, input_ids.shape[1]-1:, :]\n",
    "\n",
    "# 各トークンの確率を計算\n",
    "scores = torch.softmax(next_text_logits, dim=-1)\n",
    "for i, token_id in enumerate(generated_tokens_ids):\n",
    "  print(f'{tokenizer.decode([token_id])}: {scores[i, token_id]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na9dvEJUgP9c"
   },
   "source": [
    "## 93. パープレキシティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SI614KWcgRLY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of surprises: 99.3538\n",
      "The movies were full of surprises: 126.4832\n",
      "The movie were full of surprises: 278.8803\n",
      "The movies was full of surprises: 274.6648\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "  \"The movie was full of surprises\",\n",
    "  \"The movies were full of surprises\",\n",
    "  \"The movie were full of surprises\",\n",
    "  \"The movies was full of surprises\"\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "with torch.no_grad():\n",
    "  outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['input_ids'])\n",
    "\n",
    "# パープレキシティの計算\n",
    "shift_logits = outputs.logits[:, :-1, :].contiguous()\n",
    "shift_labels = inputs['input_ids'][:, 1:].contiguous()\n",
    "shift_mask = inputs['attention_mask'][:, 1:].contiguous()\n",
    "batch_size, seq_len = shift_labels.shape\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).view(batch_size, seq_len)\n",
    "loss = (loss * shift_mask).sum(dim=1) / shift_mask.sum(dim=1)\n",
    "ppl = torch.exp(loss).tolist()\n",
    "\n",
    "for i in range(len(texts)):\n",
    "  print(f'{texts[i]}: {ppl[i]:.4f}')\n",
    "\n",
    "# パープレキシティの計算  https://gotutiyan.hatenablog.com/entry/2022/02/23/133414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtTn1lpFgRYb"
   },
   "source": [
    "## 94. チャットテンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DQQV-4OIgSbY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dessert is typically referred to as a sweet treat that is eaten after dinner.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "\n",
    "prompt = \"What do you call a sweet eaten after dinner?\"\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512, pad_token_id=128001)\n",
    "generated_ids = [\n",
    "  output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKr1YHw1gSnP"
   },
   "source": [
    "## 95. マルチターンのチャット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plural form of \"dessert\" is \"desserts\". The word \"desserts\" spelled in reverse order is \"sretsseD\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please give me the plural form of the word with its spelling in reverse order.\"\n",
    "messages.append({\"role\": \"assistant\", \"content\":response})\n",
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512, pad_token_id=128001)\n",
    "generated_ids = [\n",
    "  output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTU_FE7FgT2x"
   },
   "source": [
    "## 96. プロンプトによる感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kau4qtt7gU8T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-01 18:21:40--  https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.230.130.72, 54.230.130.97, 54.230.130.59, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.230.130.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7439277 (7.1M) [application/zip]\n",
      "Saving to: ‘data/SST-2.zip’\n",
      "\n",
      "SST-2.zip           100%[===================>]   7.09M  35.7MB/s    in 0.2s    \n",
      "\n",
      "2025-05-01 18:21:40 (35.7 MB/s) - ‘data/SST-2.zip’ saved [7439277/7439277]\n",
      "\n",
      "Archive:  data/SST-2.zip\n",
      "   creating: data/SST-2/\n",
      "  inflating: data/SST-2/dev.tsv      \n",
      "   creating: data/SST-2/original/\n",
      "  inflating: data/SST-2/original/README.txt  \n",
      "  inflating: data/SST-2/original/SOStr.txt  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  inflating: data/SST-2/original/STree.txt  \n",
      "  inflating: data/SST-2/original/datasetSentences.txt  \n",
      "  inflating: data/SST-2/original/datasetSplit.txt  \n",
      "  inflating: data/SST-2/original/dictionary.txt  \n",
      "  inflating: data/SST-2/original/original_rt_snippets.txt  \n",
      "  inflating: data/SST-2/original/sentiment_labels.txt  \n",
      "  inflating: data/SST-2/test.tsv     \n",
      "  inflating: data/SST-2/train.tsv    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip -P data/\n",
    "!unzip -o data/SST-2.zip -d data/\n",
    "!rm data/SST-2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: packaging in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: filelock in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /net/nas5/data/home/nishida/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, charset-normalizer, attrs, async-timeout, aiohappyeyeballs, yarl, requests, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 charset-normalizer-3.4.1 datasets-3.5.1 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 requests-2.32.3 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 54.01%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ファイルの読み込み\n",
    "file_name = './data/SST-2/dev.tsv'\n",
    "df = pd.read_csv(file_name, sep='\\t')\n",
    "\n",
    "# 一文に対しての感情分析\n",
    "def sentiment_analysis(text):\n",
    "  instructions = \"\"\"\n",
    "    Please determine the positive and negative aspects of the text. \n",
    "    If it's positive, output 1, if negative, output 0.\n",
    "    You can only output 0 or 1.\n",
    "  \"\"\"\n",
    "  prompt = f\"\"\"\n",
    "    Instructions: {instructions},\n",
    "    Text: {text}\n",
    "  \"\"\"\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You can only output 0 or 1.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "  \n",
    "  text = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    "  )\n",
    "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=512, pad_token_id=128001)\n",
    "  generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "  ]\n",
    "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "  return response\n",
    "\n",
    "# 正解率の計算\n",
    "correct = 0\n",
    "for index, row in df.iterrows():\n",
    "  response = sentiment_analysis(row['sentence'])\n",
    "  if re.search(r\"\\b[01]\\b\", response) and int(re.findall(r\"\\b[01]\\b\", response)[0]) == row['label']:\n",
    "    correct += 1\n",
    "print(f\"accuracy: {correct / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca6DMpLWgWAv"
   },
   "source": [
    "## 97. 埋め込みに基づく感情分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCDrDp0AgXbC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSIf41cMgX6i"
   },
   "source": [
    "## 98. ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYLN8VTcgZH-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdGNChqpgZVA"
   },
   "source": [
    "## 99. 選好チューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0U1XT20gacr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
